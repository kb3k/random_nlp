{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8aea53b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads a corpus\n",
    "# compartmentalized toxic avenger\n",
    "# removes top n stop words - 1500? 500*10*.3\n",
    "# applies lds with 1 word, many many iterations\n",
    "# and 0 - 20 topics\n",
    "\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f2f933c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(STR):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', STR)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Example filter for noun-type structures bigrams\n",
    "\n",
    "def bigram_filter(bigram):\n",
    "    tag = nltk.pos_tag(bigram)\n",
    "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['NN']:\n",
    "        return False\n",
    "    if 'n' in bigram or 't' in bigram:\n",
    "        return False\n",
    "    if 'PRON' in bigram:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def let_bigrams_be_bigrams(string_text, finder, bigram_measures):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Example for detecting bigrams \n",
    "    \n",
    "\n",
    "    # may want to filter only those that occur at least 5 times; may not\n",
    "    #finder.apply_freq_filter(5)\n",
    "    bigram_scores = finder.score_ngrams(bigram_measures.pmi)\n",
    "    bigram_pmi = pd.DataFrame(bigram_scores)\n",
    "    bigram_pmi.columns = ['bigram', 'pmi']\n",
    "    bigram_pmi.sort_values(by='pmi', axis = 0, ascending = False, inplace = True)\n",
    "    filtered_bigram = bigram_pmi[bigram_pmi.\n",
    "                                 apply(lambda bigram: \n",
    "                                       bigram_filter(bigram['bigram']) \n",
    "                                       and bigram.pmi > 5, axis = 1)][:500]\n",
    "    bigrams = [' '.join(x) for x in filtered_bigram.bigram.values if len(x[0]) > 2 or len(x[1]) > 2]\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def replace_ngram(doc_string_text, bigrams):\n",
    "    for gram in bigrams:\n",
    "        doc_string_text = doc_string_text.replace(gram, '_'.join(gram.split()))\n",
    "    return doc_string_text\n",
    "\n",
    "\n",
    "def clean_doc(doc, finder, bigram_measures, more_stop_words=[]): \n",
    "    \"\"\"\n",
    "    Given a string doc,\n",
    "    this function cleans the doc\n",
    "    of . . .\n",
    "    non-ascii chars\n",
    "    punctuation\n",
    "    anything other than lower case words\n",
    "    non-alphabetic tokens\n",
    "    stop words\n",
    "    tokens shorter than length 3\n",
    "    stem, lemma are also options\n",
    "    returns the list of tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    bigrams = let_bigrams_be_bigrams(doc, finder, bigram_measures)\n",
    "    doc = replace_ngram(doc, bigrams)\n",
    "    \n",
    "    tokens = doc.split() # set the tone\n",
    "    \n",
    "    # punctuation situation\n",
    "    sub_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    tokens = [sub_punc.sub('', tkn) for tkn in tokens]\n",
    "    \n",
    "    # alpha\n",
    "    tokens = [tkn for tkn in tokens if tkn.isalpha()]\n",
    "    \n",
    "    # shorties\n",
    "    tokens = [tkn for tkn in tokens if len(tkn) > 3]\n",
    "    \n",
    "    # capitalization\n",
    "    tokens = [tkn.lower() for tkn in tokens]\n",
    "    \n",
    "    # stop words\n",
    "    stop_words = set(stopwords.words('english') + more_stop_words)\n",
    "    tokens = [tkn for tkn in tokens if not tkn in stop_words]\n",
    "    \n",
    "    # word stemming    \n",
    "#    ps = PorterStemmer()\n",
    "#    tokens = [ps.stem(tkn) for tkn in tokens]\n",
    "    \n",
    "    # lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(wrd) for wrd in tokens]\n",
    "    \n",
    "    # bleh??\n",
    "    # tag = nltk.pos_tag(bigram)\n",
    "    \n",
    "    return tokens # tokens is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d05e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in class corpus csv into python\n",
    "data = pd.read_csv(r'~/Projects/msds453/MSDS_453_Public/MSDS453_ClassCorpus/ClassCorpus_Final_v5_20220717.csv')\n",
    "buffer_data = data.copy()\n",
    "\n",
    "data['stripped_text'] = data.Text.apply(lambda xyz: strip_accents(xyz))\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_documents(data.stripped_text)\n",
    "\n",
    "#adding two columns to the dataframe to store the processed text and tokenized text\n",
    "buffer_data['processed_text'] = buffer_data['Text'].apply(lambda x: clean_doc(x, finder, bigram_measures))\n",
    "\n",
    "# getting the top freq qords so I can add them to stop_words\n",
    "buffer_data['word_count'] = buffer_data['processed_text'].apply(lambda xyz : [idx[0] for idx in nltk.FreqDist(xyz).most_common(int(.3*500))])\n",
    "more_stop_words = [idx[0] for idx in nltk.FreqDist([abc for xyz in buffer_data.word_count.to_list() for abc in xyz]).most_common(100)]\n",
    "\n",
    "#adding two columns to the dataframe to store the processed text and tokenized text\n",
    "data['processed_text'] = data['Text'].apply(lambda x: clean_doc(x, finder, bigram_measures, more_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4483e3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>DSI_Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Submission File Name</th>\n",
       "      <th>Student Name</th>\n",
       "      <th>Genre of Movie</th>\n",
       "      <th>Review Type (pos or neg)</th>\n",
       "      <th>Movie Title</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Doc_ID, DSI_Title, Text, Submission File Name, Student Name, Genre of Movie, Review Type (pos or neg), Movie Title, processed_text, word_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer_data[buffer_data.processed_text.apply(lambda xyz: \"_\" in ' '.join(xyz))==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7c3e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'~/Projects/msds453/MSDS_453_Public/MSDS453_ClassCorpus/ClassCorpus_Final_v5_20220717.csv')\n",
    "buffer_data = data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77959f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['stripped_text'] = data.Text.apply(lambda xyz: strip_accents(xyz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2ccea9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick.Brown fox'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"the quick.Brown fox\"\n",
    "re.sub(r\"[a-z]\\b(?!')\", lambda m: m.group().lower(), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "10c059cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick.Brown fox'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"^[A-Z]\\b(.?!)\\s\", lambda m: m.group().upper(), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "69192e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick.Brown fox'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"^[A-Z][a-z\\s]+[\\.\\?!]$\", lambda m: m.group().upper(), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a22680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "14b40ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick.brown fox'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\".[A-Z]\", lambda m: m.group().lower(), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "412a49fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"the quick. Brown fox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "093a1316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick. brown fox'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\". [A-Z]\", lambda m: m.group().lower(), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "00cc9c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick. brown fox'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"[?!.] [A-Z]\", lambda m: m.group().lower(), t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "27d0da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"the quick! Brown fox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4114c3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick? brown fox'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"[?!.] [A-Z]\", lambda m: m.group().lower(), q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f2641b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"the quick? Brown fox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37b4841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
