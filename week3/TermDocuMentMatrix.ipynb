{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "302c35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en_core_web_lg\n",
    "# warning: takes 80 years on public wifi\n",
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from typing import List, Set\n",
    "\n",
    "import spacy\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d042a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df50d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(text: str) -> str:    \n",
    "    return re.sub('&lt;/?.*?&gt;', '', text)\n",
    "\n",
    "def remove_special_chars_and_digits(text: str) -> str:\n",
    "    return re.sub('(\\\\d|\\\\W)+', '', text)\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    return re.sub('[^a-zA-Z]', '', str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736b377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemmatization(in_text):\n",
    "    # Lemmatization\n",
    "    lem = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(in_text)\n",
    "    print(word_list)\n",
    "    output = ' '.join([lem.lemmatize(w) for w in word_list])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed774f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas(text: str, stopwords: Set[str]) -> List[str]:\n",
    "    initial = [remove_tags(remove_special_chars_and_digits(remove_punctuation(x.lemma_.lower()))) for x in nlp(text)]\n",
    "    return [x for x in initial if x not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5a077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def clean_please(some_data):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    re_punk = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    lemmas = apply_lemmatization(some_data)\n",
    "    tokens = word_tokenize(lemmas)\n",
    "    stripped_tkns = [re_punk.sub('', wxy) for wxy in tokens]\n",
    "    lower = [word.lower() for word in stripped_tkns]\n",
    "    st0p = [word for word in lower if not word in stop_words]\n",
    "\n",
    "    while \"\" in st0p:\n",
    "        st0p.remove(\"\")\n",
    "    return st0p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aa17da",
   "metadata": {},
   "outputs": [],
   "source": [
    "TDM = pd.read_csv(\"../week2/document_matrix.csv\") #term_document_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89726a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"/Users/kerry/Projects/msds453/random_nlp/assign0/toxicAvenger/reviews.csv\", sep=\"|\",\n",
    "                     lineterminator=\">\", engine=\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33150819",
   "metadata": {},
   "outputs": [],
   "source": [
    "stg = \"\"\n",
    "for rev in reviews.review:\n",
    "    stg += rev\n",
    "    \n",
    "stg = stg.translate(str.maketrans('', '', string.punctuation))\n",
    "stg = stg.replace(\"\\n\", \" \").replace(\"  \", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08addbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "stg = \" \".join([word.lower() for word in stg.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f57af",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = get_lemmas(stg, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace2a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1 = FreqDist(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "TDM.rename(columns={\"Unnamed: 0\":\"terms\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eb25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['sum']=df.sum(axis=1) # jk\n",
    "TDM['n_doc_freq_of_term'] = TDM.astype(bool).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594603f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TDM.set_index(\"terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfda8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(value, idx):\n",
    "    \"\"\"\n",
    "    value is a series object passed with axis=1\n",
    "    \"\"\"\n",
    "    idf = log(10/value.n_doc_freq_of_term)+1\n",
    "    tf = value[f'Document {idx}']/len(fdist1)\n",
    "    return idf*tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624815ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = {'terms':TDM.terms,\n",
    "       'Document 1':TDM.apply(lambda x: TF_IDF(x, '1'), axis=1),\n",
    "       'Document 2':TDM.apply(lambda x: TF_IDF(x, '2'), axis=1),\n",
    "       'Document 3':TDM.apply(lambda x: TF_IDF(x, '3'), axis=1),\n",
    "       'Document 4':TDM.apply(lambda x: TF_IDF(x, '4'), axis=1),\n",
    "       'Document 5':TDM.apply(lambda x: TF_IDF(x, '5'), axis=1),\n",
    "       'Document 6':TDM.apply(lambda x: TF_IDF(x, '6'), axis=1),\n",
    "       'Document 7':TDM.apply(lambda x: TF_IDF(x, '7'), axis=1),\n",
    "       'Document 8':TDM.apply(lambda x: TF_IDF(x, '8'), axis=1),\n",
    "       'Document 9':TDM.apply(lambda x: TF_IDF(x, '9'), axis=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b4d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da12e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('terms', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c51a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c031182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664adff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = list(TDM['terms'])\n",
    "terms.remove(\"film\")\n",
    "terms.remove(\"toxic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae767e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "TDM.set_index(\"terms\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea91b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TDM2 = TDM.drop(index=[\"film\", \"toxic\"], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf899e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "abc2 = {'terms':terms,\n",
    "       'Document 1':TDM2.apply(lambda x: TF_IDF(x, '1'), axis=1),\n",
    "       'Document 2':TDM2.apply(lambda x: TF_IDF(x, '2'), axis=1),\n",
    "       'Document 3':TDM2.apply(lambda x: TF_IDF(x, '3'), axis=1),\n",
    "       'Document 4':TDM2.apply(lambda x: TF_IDF(x, '4'), axis=1),\n",
    "       'Document 5':TDM2.apply(lambda x: TF_IDF(x, '5'), axis=1),\n",
    "       'Document 6':TDM2.apply(lambda x: TF_IDF(x, '6'), axis=1),\n",
    "       'Document 7':TDM2.apply(lambda x: TF_IDF(x, '7'), axis=1),\n",
    "       'Document 8':TDM2.apply(lambda x: TF_IDF(x, '8'), axis=1),\n",
    "       'Document 9':TDM2.apply(lambda x: TF_IDF(x, '9'), axis=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d044247",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(abc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6871eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.set_index('terms', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd730757",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.idxmax()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
